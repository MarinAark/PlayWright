"""
æ–‡ä»¶ä¸‹è½½å·¥å…·
æ”¯æŒä»Excelæ–‡ä»¶è¯»å–ä¸‹è½½é“¾æ¥å¹¶æ‰¹é‡ä¸‹è½½æ–‡ä»¶
"""
import os
import logging
import requests
import pandas as pd
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from urllib.parse import urlparse
import time

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class FileDownloader:
    """æ–‡ä»¶ä¸‹è½½å·¥å…·ç±»"""
    
    def __init__(self, save_dir: str = "./downloads", timeout: int = 30, max_retries: int = 3):
        self.save_dir = Path(save_dir)
        self.save_dir.mkdir(exist_ok=True)
        self.timeout = timeout
        self.max_retries = max_retries
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.downloaded_count = 0
        self.skipped_count = 0
        self.failed_count = 0
        
        # è¯·æ±‚ä¼šè¯ï¼Œæ”¯æŒè¿æ¥å¤ç”¨
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    def sanitize_filename(self, name: str) -> str:
        """æ¸…ç†æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦"""
        # ç§»é™¤æˆ–æ›¿æ¢éæ³•å­—ç¬¦
        illegal_chars = r'\/:*?"<>|'
        for char in illegal_chars:
            name = name.replace(char, '_')
        
        # ç§»é™¤å‰åç©ºæ ¼å’Œç‚¹
        name = name.strip('. ')
        
        # é™åˆ¶é•¿åº¦
        if len(name) > 200:
            name = name[:200]
        
        return name
    
    def extract_filename_from_url(self, url: str) -> str:
        """ä»URLä¸­æå–æ–‡ä»¶å"""
        try:
            parsed = urlparse(url)
            filename = os.path.basename(parsed.path)
            if filename and '.' in filename:
                return filename
        except Exception:
            pass
        
        # å¦‚æœæ— æ³•æå–ï¼Œä½¿ç”¨é»˜è®¤åç§°
        return "downloaded_file"
    
    def get_file_extension_from_url(self, url: str) -> str:
        """ä»URLä¸­æå–æ–‡ä»¶æ‰©å±•å"""
        try:
            parsed = urlparse(url)
            path = parsed.path
            if '.' in path:
                return os.path.splitext(path)[1]
        except Exception:
            pass
        return ""
    
    def download_single_file(
        self, 
        url: str, 
        filename: str, 
        custom_filename: Optional[str] = None
    ) -> bool:
        """
        ä¸‹è½½å•ä¸ªæ–‡ä»¶
        
        Args:
            url: ä¸‹è½½é“¾æ¥
            filename: åŸå§‹æ–‡ä»¶å
            custom_filename: è‡ªå®šä¹‰æ–‡ä»¶å
            
        Returns:
            bool: æ˜¯å¦ä¸‹è½½æˆåŠŸ
        """
        if not url or url.strip() == "":
            logger.warning("URLä¸ºç©ºï¼Œè·³è¿‡")
            return False
        
        url = url.strip()
        
        # ç¡®å®šæœ€ç»ˆæ–‡ä»¶å
        if custom_filename:
            final_filename = self.sanitize_filename(custom_filename)
        else:
            final_filename = self.sanitize_filename(filename)
        
        # å¦‚æœæ–‡ä»¶åæ²¡æœ‰æ‰©å±•åï¼Œå°è¯•ä»URLè·å–
        if not os.path.splitext(final_filename)[1]:
            ext = self.get_file_extension_from_url(url)
            if ext:
                final_filename += ext
        
        filepath = self.save_dir / final_filename
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
        if filepath.exists():
            logger.info(f"æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡: {final_filename}")
            self.skipped_count += 1
            return True
        
        # ä¸‹è½½æ–‡ä»¶
        for attempt in range(self.max_retries):
            try:
                logger.info(f"æ­£åœ¨ä¸‹è½½: {final_filename} (å°è¯• {attempt + 1}/{self.max_retries})")
                
                response = self.session.get(url, stream=True, timeout=self.timeout)
                response.raise_for_status()
                
                # è·å–æ–‡ä»¶å¤§å°
                total_size = int(response.headers.get('content-length', 0))
                
                with open(filepath, "wb") as f:
                    downloaded = 0
                    for chunk in response.iter_content(chunk_size=8192):
                        if chunk:
                            f.write(chunk)
                            downloaded += len(chunk)
                            
                            # æ˜¾ç¤ºè¿›åº¦ï¼ˆæ¯10MBæ˜¾ç¤ºä¸€æ¬¡ï¼‰
                            if total_size > 0 and downloaded % (10 * 1024 * 1024) == 0:
                                progress = (downloaded / total_size) * 100
                                logger.info(f"ä¸‹è½½è¿›åº¦: {final_filename} - {progress:.1f}%")
                
                file_size = filepath.stat().st_size / (1024 * 1024)  # MB
                logger.info(f"ä¸‹è½½å®Œæˆ: {final_filename} ({file_size:.2f} MB)")
                self.downloaded_count += 1
                return True
                
            except requests.exceptions.RequestException as e:
                logger.warning(f"ä¸‹è½½å¤±è´¥ (å°è¯• {attempt + 1}): {final_filename} - {e}")
                if attempt < self.max_retries - 1:
                    time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                else:
                    logger.error(f"ä¸‹è½½æœ€ç»ˆå¤±è´¥: {final_filename}")
                    self.failed_count += 1
                    # åˆ é™¤å¯èƒ½çš„ä¸å®Œæ•´æ–‡ä»¶
                    if filepath.exists():
                        filepath.unlink()
                    return False
            
            except Exception as e:
                logger.error(f"ä¸‹è½½å¼‚å¸¸: {final_filename} - {e}")
                self.failed_count += 1
                if filepath.exists():
                    filepath.unlink()
                return False
        
        return False
    
    def download_from_excel(
        self, 
        excel_path: str, 
        url_column: str = "æ–‡ä»¶é“¾æ¥",
        name_column: str = "æ–‡ä»¶åç§°", 
        status_column: str = "æ¸…æ´—çŠ¶æ€",
        skip_cleaned: bool = True
    ) -> Tuple[int, int, int]:
        """
        ä»Excelæ–‡ä»¶è¯»å–ä¸‹è½½åˆ—è¡¨å¹¶æ‰¹é‡ä¸‹è½½
        
        Args:
            excel_path: Excelæ–‡ä»¶è·¯å¾„
            url_column: URLåˆ—å
            name_column: æ–‡ä»¶ååˆ—å
            status_column: çŠ¶æ€åˆ—å
            skip_cleaned: æ˜¯å¦è·³è¿‡å·²æ¸…æ´—çš„æ–‡ä»¶
            
        Returns:
            Tuple[int, int, int]: (ä¸‹è½½æ•°é‡, è·³è¿‡æ•°é‡, å¤±è´¥æ•°é‡)
        """
        excel_path = Path(excel_path)
        if not excel_path.exists():
            raise FileNotFoundError(f"Excelæ–‡ä»¶ä¸å­˜åœ¨: {excel_path}")
        
        # é‡ç½®ç»Ÿè®¡
        self.downloaded_count = 0
        self.skipped_count = 0
        self.failed_count = 0
        
        try:
            # å°è¯•ä½¿ç”¨ä¸åŒçš„å¼•æ“è¯»å–Excel
            engines = ['calamine', 'openpyxl', 'xlrd']
            df = None
            
            for engine in engines:
                try:
                    df = pd.read_excel(excel_path, engine=engine)
                    logger.info(f"ä½¿ç”¨ {engine} å¼•æ“æˆåŠŸè¯»å–Excelæ–‡ä»¶")
                    break
                except Exception as e:
                    logger.warning(f"ä½¿ç”¨ {engine} å¼•æ“è¯»å–å¤±è´¥: {e}")
                    continue
            
            if df is None:
                raise ValueError("æ— æ³•è¯»å–Excelæ–‡ä»¶ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼")
            
            logger.info(f"Excelæ–‡ä»¶åŒ…å« {len(df)} è¡Œæ•°æ®")
            
            # æ£€æŸ¥å¿…éœ€çš„åˆ—
            missing_columns = []
            for col in [url_column, name_column]:
                if col not in df.columns:
                    missing_columns.append(col)
            
            if missing_columns:
                raise ValueError(f"Excelæ–‡ä»¶ç¼ºå°‘å¿…éœ€çš„åˆ—: {missing_columns}")
            
            # å¤„ç†æ¯ä¸€è¡Œ
            for idx, row in df.iterrows():
                try:
                    url = str(row[url_column]).strip() if pd.notna(row[url_column]) else ""
                    name = str(row[name_column]).strip() if pd.notna(row[name_column]) else ""
                    status = str(row[status_column]).strip() if pd.notna(row.get(status_column)) else ""
                    
                    if not url or not name:
                        logger.warning(f"ç¬¬{idx+2}è¡Œæ•°æ®ä¸å®Œæ•´ï¼Œè·³è¿‡")
                        continue
                    
                    # è·³è¿‡å·²æ¸…æ´—çš„æ–‡ä»¶
                    if skip_cleaned and status == "å·²æ¸…æ´—":
                        logger.info(f"ç¬¬{idx+2}è¡Œå·²æ¸…æ´—ï¼Œè·³è¿‡: {name}")
                        self.skipped_count += 1
                        continue
                    
                    # ç”Ÿæˆæ–‡ä»¶å
                    status_label = status if status else "æœªæ¸…æ´—"
                    custom_filename = f"{idx+2}_{name}_{status_label}"
                    
                    # ä¸‹è½½æ–‡ä»¶
                    self.download_single_file(url, name, custom_filename)
                    
                except Exception as e:
                    logger.error(f"å¤„ç†ç¬¬{idx+2}è¡Œæ—¶å‡ºé”™: {e}")
                    self.failed_count += 1
                    continue
        
        except Exception as e:
            logger.error(f"è¯»å–Excelæ–‡ä»¶å¤±è´¥: {e}")
            raise
        
        return self.downloaded_count, self.skipped_count, self.failed_count
    
    def get_statistics(self) -> Dict[str, int]:
        """è·å–ä¸‹è½½ç»Ÿè®¡ä¿¡æ¯"""
        total = self.downloaded_count + self.skipped_count + self.failed_count
        return {
            'downloaded': self.downloaded_count,
            'skipped': self.skipped_count,
            'failed': self.failed_count,
            'total': total,
            'success_rate': (self.downloaded_count / max(1, total)) * 100
        }
    
    def close(self):
        """å…³é—­ä¼šè¯"""
        self.session.close()


def main():
    """ä¸»å‡½æ•°ï¼Œç”¨äºå‘½ä»¤è¡Œæ‰§è¡Œ"""
    import argparse
    
    parser = argparse.ArgumentParser(description="æ–‡ä»¶æ‰¹é‡ä¸‹è½½å·¥å…·")
    parser.add_argument("excel_file", help="Excelæ–‡ä»¶è·¯å¾„")
    parser.add_argument("-d", "--dir", default="./downloads", help="ä¸‹è½½ç›®å½•ï¼ˆé»˜è®¤ï¼š./downloadsï¼‰")
    parser.add_argument("--url-column", default="æ–‡ä»¶é“¾æ¥", help="URLåˆ—åï¼ˆé»˜è®¤ï¼šæ–‡ä»¶é“¾æ¥ï¼‰")
    parser.add_argument("--name-column", default="æ–‡ä»¶åç§°", help="æ–‡ä»¶ååˆ—åï¼ˆé»˜è®¤ï¼šæ–‡ä»¶åç§°ï¼‰")
    parser.add_argument("--status-column", default="æ¸…æ´—çŠ¶æ€", help="çŠ¶æ€åˆ—åï¼ˆé»˜è®¤ï¼šæ¸…æ´—çŠ¶æ€ï¼‰")
    parser.add_argument("--include-cleaned", action="store_true", help="åŒ…æ‹¬å·²æ¸…æ´—çš„æ–‡ä»¶")
    parser.add_argument("--timeout", type=int, default=30, help="ä¸‹è½½è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼Œé»˜è®¤30ï¼‰")
    parser.add_argument("--max-retries", type=int, default=3, help="æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆé»˜è®¤3ï¼‰")
    parser.add_argument("-v", "--verbose", action="store_true", help="è¯¦ç»†è¾“å‡º")
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    downloader = FileDownloader(
        save_dir=args.dir,
        timeout=args.timeout,
        max_retries=args.max_retries
    )
    
    try:
        downloaded, skipped, failed = downloader.download_from_excel(
            excel_path=args.excel_file,
            url_column=args.url_column,
            name_column=args.name_column,
            status_column=args.status_column,
            skip_cleaned=not args.include_cleaned
        )
        
        stats = downloader.get_statistics()
        
        print(f"\nğŸ“Š ä¸‹è½½ç»Ÿè®¡:")
        print(f"  æ€»æ–‡ä»¶æ•°: {stats['total']}")
        print(f"  å·²ä¸‹è½½: {stats['downloaded']}")
        print(f"  å·²è·³è¿‡: {stats['skipped']}")
        print(f"  ä¸‹è½½å¤±è´¥: {stats['failed']}")
        print(f"  æˆåŠŸç‡: {stats['success_rate']:.1f}%")
        
        if failed > 0:
            return 1
    
    except Exception as e:
        logger.error(f"æ‰§è¡Œå¤±è´¥: {e}")
        return 1
    
    finally:
        downloader.close()
    
    return 0


if __name__ == "__main__":
    exit(main())
